# 必备数学基础

### 高等数学基础

#### 函数

> **WHAT**:后面基本都是用函数，这里先理解一下函数的概念

**函数的定义**：

- y = f(x) 其中x是自变量，y是因变量。y随着x变化

**几种特性**：

奇偶性、周期性、单调性（如下图）

![1603799800751](assets/1603799800751.png)

**极限**：

- 按照一定次数排列的数：x1，x2，...，xn，其中xn叫做通项
- 对于数列｛xn｝,当n无限增大时，其通项无限接近于一个常数A，则称该数列以A为极限或称数列收敛于A。

**导数**：

- 都有对应的结果，不用死记硬背，查就行了，如(C)' = 0 或者(sin x)' = cos x



#### 方向导数（引出梯度）

> 在函数定义域的内点，对某一*方向*求导得到的*导数*。
>
> 常规数学中，所有问题都有一个解。而机器学习当中，求解很难或者没有解，我们只能不断逼近这个最优解。

**问题一**：蚂蚁沿着什么方向跑路不被火烧，能活下来（二维平面）

![有个坐标轴x,y，(0,0)处着火，蚂蚁应该怎么走](assets/1603799891825.png)

> 蚂蚁沿着任意方向都可以活，最优的是沿着对角方向L，z是函数变化，也就是图中的φ。

**三维平面的方向导数公式**：

![1603799859450](assets/1603799859450.png)



**求一个方向导数具体的值**：

求函数![1603800015017](assets/1603800015017.png)在点P(1,0)处，沿着从点P(1,0)到点Q(2,-1)的方向的方向导数。

![1603800127515](assets/1603800127515.png)



所求方向导数

![1603800171837](assets/1603800171837.png)

#### 梯度

> **WHAT**:简而言之，就是找到函数在某点沿着哪个梯度方向变化最大（小），也就是怎样的方向对数据逼近所需要的值最好。
>
> 是一个向量（矢量），表示某一函数在该点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向（此*梯度*的方向）变化最快，变化率最大（为该*梯度*的模）。

函数：z = f(x,y)在平面域内具有连续的一阶偏导数，对于其中每个点P(x,y)都有向量![1603800802065](assets/1603800802065.png)则其称为函数点P的梯度。

![1603800856376](assets/1603800856376.png)

![1603800888757](assets/1603800888757.png)是方向L上的单位向量

![1603800922280](assets/1603800922280.png)

![1603800960729](assets/1603800960729.png)

> 根据上面的梯度导数，和方向导数的区别就在多了个*cosθ*，*θ*充当梯度和方向导数之间的关系

只有当![1603801027540](assets/1603801027540.png)才有最大值

函数在某点的梯度是一个向量，它的方向与方向导数最大值取得的方向一致。

其大小正好是最大的方向导数

![梯度图](assets/1603681846373.png)

> 注意，只有*θ*=0，*cos*导数才能=1，梯度才能取得最大值，也就是那个方向。而沿着反方向就是最小值也就是梯度下降。

**求一个具体值，最大梯度方向和最小梯度方向**：

设![1603800305729](assets/1603800305729.png)求grad u，并求在点M(0,1,-1)处方向导数的最大（小）值

![1603800371917](assets/1603800371917.png)

![1603800394319](assets/1603800394319.png)

![1603800457473](assets/1603800457473.png)

> 注：得出的结果(-1,0,2)，求解：((-1^2) + (0^2) + (-2^2)) = √5，前面都是x的平方，所以结果也需要开根号。



### 微积分

#### 微积分基本理论

> **WHAT**:前面说到，机器学习当中，求解很难或者没有解，而微积分也是一个用简单的方式，求一个与实际情况最接近的答案。
>
> 很多的微分积起来

如何求A面积的值

![1603589223245](assets/1603589223245.png)

**以直代曲**：

- 对于矩形，我们可以轻松求得其面积，能否用矩形代替曲线形状呢？


- 应该用多少个矩形来代替？


![四个小矩形和九个小矩形](assets/1603685656784.png)

> 越小的矩形，越覆盖，然后求每个矩形的面积。

**面积的由来**：

- 在ab之间插入若干个点，这样就得到n个小区间。
- 每个小矩形面积为：![1603801255298](assets/1603801255298.png)近似得到曲线面积![1603801287337](assets/1603801287337.png)
- 当分割无限加细，每个小区间的最大长度为λ，此时λ → 0
- 曲边面积：![1603801393606](assets/1603801393606.png)

![1603688411669](assets/1603688411669.png)

> 注意每个小区间的最大长度为λ，而λ无限接近于0时，那么曲边的面积我们就可以得出，当然这里的近似表达是极限，无限接近的极限。

**求和**：

我们需要尽可能的将每一个矩形的底边无穷小



莱布尼茨为了体现求和的感觉，把S拉长了，简写成![1603790307464](assets/1603790307464.png)

![1603765637923](assets/1603765637923.png)

> 将上面的所有矩阵求和，∫ = sum，求和的意思

**定积分**:

当![1603790249795](assets/1603790249795.png)时，总和S总数趋于确定的极限l，则称极限l为函数f(x)在曲线[a,b]上的定积分

![1603765921296](assets/1603765921296.png)



### 泰勒公式

> **what**：用简单、熟悉的多项式来近似替代复杂的函数。
>
> 一个概念可以自己去找找，需要就找我，我再把内容加上



### 线性代数基础

#### 矩阵和特征

> **WHAT**：人工智能领域，数据基本是矩阵形式，而矩阵的每列（一般是除开首列），称为特征

**矩阵**：

> 拿到数据后，数据就长如下样子，有行有列

![1603615232363](assets/1603615232363.png)

> 左图√表示A可以到B和C，如右上图，再把√号改成0/1以存储在数据里面，就如右下图

**几种特别的矩阵**：

![1603790184301](assets/1603790184301.png)

> 上三角部分有值，和下三角部分有值

![1603790200046](assets/1603790200046.png)

> 对角阵：对角有值且可以是任意值，单位矩阵：对角有值且相同

![1603790209907](assets/1603790209907.png)

> 同型矩阵：行列相同。矩阵相等：行列相同且里面的值一样

#### 向量内积

- 设有n维向量：![1603802689962](assets/1603802689962.png)

- [x, y] = x1 y1 + x2 y2 + ... + xn yn，此时我们就把[x,y]叫做向量的内积。
- ![1603802766445](assets/1603802766445.png)
  - 对称性：[x, y] = [y, x]
  - 线性性质：[λx, y] = λ[x, y], [x + y, z] = [x, z] + [y, z]

#### SVD矩阵分解

> **WHAT**：为了让数据的呈现更好，且不破坏数据的原始表达

数据行列可能很大，如电商行业100万客户（行），有1万的商品（特征），用一组数据表达就是

| 客户ID   | 商品1             | 商品2 | ...  | 商品1万 |
| -------- | ----------------- | ----- | ---- | ------- |
| xxx1     | 1（表示买过一次） | 0     | ...  | 5       |
| xxx2     | 0                 | 1     | ...  | 0       |
| ...      | 5                 | 10    | ...  | 0       |
| xxx100万 | ...               | ...   | ...  | ...     |

那么来一个客户，就是直接多1万列表示，这样的数据是非常稀疏的，我们可以分解成A表100万客户，100个特征，而这100个特征对应这那B表的1万个商品，也就是一个表变成A表和B表，且两者关联。

这就需要用到SVD矩阵。



### 随机变量

#### 离散和连续型数据

![1603623698138](assets/1603623698138.png)

> 离散型是有限多个的，比如10个台阶，只可能是其中的一个台阶，一个确定的结果。
>
> 连续型则可能是任意的值，没办法确定是哪个台阶。

**离散型随机变量概率分布**

- 找到离散型随机变量X的所有可能取值

- 得到离散型随机变量取这些值的概率

  ![1603767423885](assets/1603767423885.png)

  ![1603790123695](assets/1603790123695.png)为离散型随机变量的概率函数

**连续型随机变量概率分布**

- 密度：一个物体，如果问其中一个点的质量是多少？这该怎么求？

  由于这个点实在太小了，那么质量就为0了，但是其中的一大块是由

  很多个点组成的，这时我们就可以根据密度来求其质量了

- X为连续随机变量，X在任意区间(a,b]上的概率可以表示为：

  ![1603790041924](assets/1603790041924.png)其中f(x)就叫做X的概率密度函数，也可以简单叫做密度

> 还有一种方法是把每个值划分在不同区间，变成离散型，但如果有新数据进来就要再划分区间导致区间越来越多。

#### 简单随机抽样

抽取的样本满足两点

1. 样本X1，X2...Xn是相互独立的随机变量。

2. 样本X1，X2...Xn与总体X同分布。

   ![1603790015180](assets/1603790015180.png)

#### 极大似然估计

> **WHAT**：找到最有可能的结果

1. 构造似然函数：L(θ)

2. 对似然函数取对数：lnL(θ)

   > 做log后，logAB = logA + logB，加法更好求

3. 求偏导![1603801570385](assets/1603801570385.png)

4. 求解得到 θ 值

   ![1603768031523](assets/1603768031523.png)

> 第一步构造函数；第二步取对数，对数后的值容易取且极值点还是那个位置；第三步求偏导；得到θ

**求一个具体的值**：

设 X 服从参数 λ(λ>0) 的泊松分布，x1,x2,...,xn 是来自 X 的一个样本值，求λ的极大似然估计值

- 因为X的分布律为![1603802012244](assets/1603802012244.png)
- 所以 λ 的似然函数为![1603802070909](assets/1603802070909.png)
- ![1603802228693](assets/1603802228693.png)
- 令![1603802263577](assets/1603802263577.png)
- 解得 λ 的极大似然估计值为 ![1603802356318](assets/1603802356318.png)



### 概率论基础

#### 概率与频率

- 抛硬币和王者游戏击杀数，这些都是随机的
- 其特点：可以在相同条件下重复执行、事先知道可能出现的结果、开始前并不知道这一次的结果
- 随机试验E的所有结果构成的集合称为E的样本空间 S = {e}
  - 抛硬币：S = {正面，反面}
  - 击杀数：S = {0,1,2,...,n}

**频率与概率**

- A在这N次试验中发生的频率：![1604149872208](assets/1604149872208.png)，其中![1604149927547](assets/1604149927547.png)发生的次数(频数)；n—总试验次数。
- ![1604150013285](assets/1604150013285.png)的稳定值P定义为A的概率P(A) = p

- 次数越多则结果越稳定

#### 古典概型

- 定义：试验E中样本点是有限的，出现每一样本点的概率是相同。

  P(A) = A所包含的样本点数 / S中的样本点数

- 一袋中有8个球，编号为1 - 8，其中1 - 3号为红球，4 - 8 为黄球，设摸到每一球的可能性相等，从中随机摸一球，记A={摸到红球}，求P(A)。

  - S={1,2,...,8}
  - A={1,2,3} => P(A) = 3/8

#### 条件概率

> **WHAT**：在一定条件下的某个事件发生的概率

- 有个不放回的抽奖，一共三种可能性，两个不中奖一个中奖，也就是3个人抽奖，必有一个中奖，所有可能为{YNN, NYN, NNY}，N表示不中间，Y表示中间
- 第一名没中则：A = {NYN, NNY}，第三中的概率![1604157828202](assets/1604157828202.png)
- 样本空间变了，概率也变了

#### 独立性

> **WHAT**：两个或多个随机事件的发生概率不相互影响。

例题：

甲、乙两人同时向一个目标射击，甲击中率为0.8，乙击中率为0.7，求目标被击中的概率。

设A={甲击中}，B={乙击中}，C={目标被击中}

则：C = A ∪ B，P(C) = P(A) + P(B) - P(AB)![1604327284928](assets/1604327284928.png)

∵ 甲、乙同时射击，其结果互不影响，

∴ A, B相互独立

P(C) = 0.7+0.8-0.56 = 0.94

#### 二维随机变量

> **WHAT**:关心两个指标并了解其相互关系

如：为了了解学生的身体状况，观察学生的身高（X）和体重（Y）及两者的相互关系

- 有二维离散型随机变量
- 有二维连续型随机变量

#### 期望

> **WHAT**：期望达到什么，反映了随机变量的取值水平

- 离散型随机变量X的分布律为：![1604328684174](assets/1604328684174.png)

  若级数![1604328741718](assets/1604328741718.png)绝对收敛，则称其为随机变量X的数学期望，![1604328798712](assets/1604328798712.png)

  > Xk是每种情况，Pk是每种情况对应的概率

  - 投骰子的期望则是1 / (1/6) + 2 / (1/6) + ... + 6 / (1/6) = 21 / 6 = 3.5

- 连续型随机变量X的概率密度为f(x)，若积分![1604329061894](assets/1604329061894.png)绝对收敛，则称积分的值![1604329099005](assets/1604329099005.png)为随机变量X的数学期望。![1604329148959](assets/1604329148959.png)

  - 随机变量X满足于均匀分布，求其期望。

    ![1604329225421](assets/1604329225421.png)=>![1604329263954](assets/1604329263954.png)

    ![1604329379855](assets/1604329379855.png)

**方差**

> 衡量随机变量相对于数学期望的分散程度



#### 贝叶斯拼写纠错

问题：我们看到用户输入了一个不在字典中的单词，我们需要去猜测用户到底想输入的是什么

- P(猜测想输入的单词|用户实际输入的单词)
- 用户实际输入的单词记为D（D代表Data，即观测数据）
- 猜测1：P(h1|D)，猜测2：P(h2|D)，猜测3：P(h3|D) ...
- P(h|D) = P(h) * P(D|h) / P(D)

> p(h) 在字典里某个词出现的次数占总体的比（先验概率）
>
> P(D|h)指输入一个词，输错的概率多大；
>
> P(D)客户输入的值D，可以约掉

贝叶斯方法计算：P(h|D) = P(h) * P(D|h) ，P(h)是特定猜测的先验概率。

比如用户输入tlp，到底是top还是tip？当最大似然不能作出决定性判断时（可能两边都是一半可能性），这是先验概率就可以插手给出指示，告诉我们，一般来说top出现的程度要高许多，所以他更可能想打的是top。



#### 垃圾邮件过滤

模型比较理论

- 最大似然：最符合观测数据的（即P(D|h)最大的）最有优势
- 奥卡姆剃刀：P(h)较大的模型有较大的优势
- 抛一枚硬币，观察到的是“字”，根据最大似然估计的理念，我们应该猜测这枚硬币抛出“字”的概率是1，因为这个才能最大化P(D|h)的猜测

实例：

- 问题：给定一封邮件，判定它是否属于垃圾邮件

  D来表示这封邮件，注意D是由N个单词组成。

  我们用h+表示垃圾邮件，h-表示正常邮件

- P(h+|D) = P(h+) * P(D|h+) / P(D)

  P(h-|D) = P(h-) * P(D|h-) / P(D)

  > P(h+)是先验概率，只需要计算一个邮件库垃圾邮件和正常邮件的比例；
  >
  > P(D|h+) 垃圾邮件中，目前这封邮件里面的词有多少个相似。D里面含有N个单词d1，d2，d3，P(D|h+) =P(d1,d2,...,dn|h+)，扩展：P(d1|h+)  * P(d2|d1,h+)  * P(d3|d2,d1, h+)* ...，垃圾邮件第一个词是d1的概率 * 垃圾邮件第一个词是d1 第二个词是d2的概率 * 垃圾邮件第一个词是d1第二个词是d2第三个词是d3的概率...

- 上面的公式太麻烦了，例用朴素贝叶斯简化，朴素贝叶斯假设特征之间是独立，互不影响的。这么假设完d1，d2，d3完全没关系了，

  简化为P(d1|h+)  * P(d2|h+) * P(d3|h+)  * ...

- 对于P(d1|h+)  * P(d2|h+) * P(d3|h+)  * ... 只要统计di这个词在垃圾邮件中出现的概率。如：全部100封邮件中，di个词出现的概率

- 再回到最上面 P(h+|D) = P(h+) * P(D|h+) / P(D)，P(D)正常异常相同，一起省略，P(h+)是先验概率，P(D|h+) 是该封信的每个词在垃圾邮件中出现的概率，这样就可以得到结果了



### 数据科学的几种分布

#### 正态分布

> 代表宇宙中大多数的运转状态，大量的随机变量被证明是正态分布的。

若随机变量X服从一个数学期望为μ、方差为σ^2的正态分布，记为N(μ, σ^2)。其概率密度函数为正态分布的期望值μ决定了其位置，其标准差σ决定了分别的幅度。当μ = 0，σ = 1时的正态分布是标准正态分布。

- 公式![1604500626232](assets/1604500626232.png)

  μ是均值

  σ是标准差

![正态分布图](assets/1604500841697.png)

#### 二项式分布

> 结果只有两个

投篮只有进球或者不进球，进球概率可能是0.5也可能不是，而不进球概率 = 1 - 进球概率。

二项式得属性包括：

- 每个试验都是独立的。
- 试验中的结果只有两种可能：进球和不进球。
- 总共进行了n次相同得试验。
- 所有试验进球和不进球的概率是相同的。

公式![1604501372142](assets/1604501372142.png)

N * p表示分布的均值

#### 泊松分布

适用于在随机时间和空间上发生事件的情况，其中，我们只关注事件发生的次数，如：

- 医院在一天内录制的紧急电话的数量
- 某个地区在一天内报告的失窃的数量
- 在特定城市上报自杀的人数

当以下假设有效时，则称为泊松分布

- 任何一个成功的事件都不应该影响另一个成功的事件
- 在短时间内成功的概率必须等于在更长时间内成功的概率
- 时间间隔很小时，在给间隔时间内成功的概率趋向于零

泊松分布中使用的符号

- λ是事件发生的速率
- t是时间间隔的长
- X是该时间间隔内的事件数
- 其中，X称为泊松随机变量，X的概率分布称为泊松分布
- 令μ表示长度为t的间隔中的平均事件数。μ = λ * t

公式![1604502244229](assets/1604502244229.png)

**求一个具体的值**

- 已知平均每小时出生3个婴儿，请问接下来的两小时，一个婴儿都不出生的概率？

  描述某段时间内，事件具体的发生概率

  ![1604502577809](assets/1604502577809.png)

- P表示概率，N表示某种函数关系，t表示时间，n表示数量，1小时内出生3个婴儿的概率，就表示为P(N(1)=3)，λ是事件的频率。

  ![1604502507252](assets/1604502507252.png)

#### 均匀分布

对于骰子来说，结果是1到6，得到任何一个结果的概率是相等的，这就是均匀分布的基础。与伯努利分布不同，均匀分布的所有看你结果的n个数都是相等的。

如果变量X是均匀分布的，则密度曲线可以表示为：![1604580275940](assets/1604580275940.png)                             ![1604580295860](assets/1604580295860.png)

均匀分布的曲线：

![1604580312335](assets/1604580312335.png)

均与分布曲线是一个矩形，又称为矩形分布。

**求一个具体的值**：

花店每天销售的花束数量是均匀分布的，最多40，最少为10，求日销量在15到30之间的概率。

日销量在15到30之间的概率为(30-15)*(1/(40-10)) = 0.5

也可求日销量大于20的概率为 0.667

#### 卡方分布

> 通过小数量的样本容量取预估总体容量的分布情况

卡方验证统计样本的实际观测值与理论推断值之间的偏离程度

公式![1604582754737](assets/1604582754737.png)

where ![1604582794426](assets/1604582794426.png)

#### Beta分布

> 一个概率的概率分布，当不知道一个东西的具体概率时，可以给出所有概率的可能性大小

举一个简单的例子，熟悉棒球运动的都知道有一个指标就是棒球击球率(batting average)，就是用一个运动员击中的球数除以击球的总数，我们一般认为0.266是正常水平的击球率，而如果击球率高达0.3就被认为是非常优秀的。

现在有一个棒球运动员，我们希望能够预测他在这一赛季中的棒球击球率是多少。你可能就会直接计算棒球击球率，用击中的数除以击球数，但是如果这个棒球运动员只打了一次，而且还命中了，那么他就击球率就是100%了，这显然是不合理的，因为根据棒球的历史信息，我们知道这个击球率应该是0.215到0.36之间才对。

最好的方法来表示这些经验（在统计中称为先验信息）就是用beta分布，这表示在我们没有看到这个运动员打球之前，我们就有了一个大概的范围。beta分布的定义域是(0,1)这就跟概率的范围是一样的。

接下来我们将这些先验信息转换为beta分布的参数，我们知道一个击球率应该是平均0.27左右，而他的范围是0.21到0.35，那么根据这个信息，我们可以取α=81,β=219。

之所以取这两个参数是因为：

- beta分布的均值是从图中可以看到分布主要落在(0.2,0.35)间，这是经验中得出的合理范围
- 在这个例子中，x轴就表示各个击球率的取值，x对应的y值就是这个击球率对应的概率。也就是beta分布可以看作一个概率的概率分布

![1604584217722](assets/1604584217722.png)

- α和β是一开始的参数，在这里是81和219。当α增加了1（击中一次）。β没有增加（没有漏球）。这就是我们新的beta分布Beta(81+1,219)。
- 当得到了更多的数据，假设一共打了300次，其中击中100，200次没击中，那么新的分布就是Beta(81+100,219+200)

![1604584439405](assets/1604584439405.png)

根据公式 α / (α+β) = (82+100) / (82+100+219+200) = 0.303，命中率提升了，蓝色曲线右移。



### 核函数

#### 核函数的目的

> 最基本的出发点是升维，使得数据更好一些，更多一些

核函数是SVM支持向量机当中最重要的函数

出发点

- 如果数据有足够多的可利用的信息，那么可以直接做想要的事情。但是现在没有那么多的信息，我们可不可以在数学上进行一些投机呢？

- 低维（比如我只知道一个人的年龄，性别，那我们能对他有更多了解吗）

  高维（比如我知道从他出生开始，做过哪些事，赚过哪些钱等）

- 如果我们对数据更好的了解，得到的结果也会更好（机器也是一样）

![1604585252689](assets/1604585252689.png)

> 上图中，我们很难说画一个圈来区分红点和绿点，一般画直线或者曲线，如果我们把二维转换成三维，我们只需要一个面就可以切分开了，低维很难解决的问题，高维能很容易解决。核函数就是解决这么一个问题

低维的数据变成高维后，数据量和计算量也会有所增加，引出下面的解决方法。

#### 线性核函数

- Linear核函数对数据不做任何变换。![1604671995246](assets/1604671995246.png)

- 何时用，特征已经比较丰富，样本数据量巨大，需要进行实时得出结果的问题
  - 越复杂的模型，针对的数据集越窄，泛化能力越差，且处理速度很慢，当然越复杂也代表着越强大。
- 不需要设置任何参数，直接就可以用

#### 多项式核函数

- 需要给定3个参数![1604673235564](assets/1604673235564.png)

  > Q越大，越复杂

- 一般情况下2次的更常见![1604673291749](assets/1604673291749.png)

- γ(gama)对内积进行缩放，ζ(zeta)控制常数项，Q控制高次项。

  其特例就是线性核函数

#### 核函数实例

还是先从一个小例子来阐述问题。假设我们有俩个数据,x=(x1,x2,x3);y=(y1,y2,y3)，此时在3D空间已经不能对其进行线性划分，那么我们通过一个函数将数据映射到更高维的空间，比如9维的话，那么(x)=(x1x2,x1x2,x1x3,x2x1,X2x2,x2x3,x3x1,x3x2,x3x3)，由于需要计算内积,所以在新的数据在9维空间，需要计算<fx),f(y)>的内积,需要花费O(n^2)。

再具体点，令x = (1,2,3); y = (4,5,6), 那么f(x) = (1,2,3,2,4,6,3,6,9)，计算方式如上的x1x2内积相乘，f(y) = (16,20,24,20,25,36,24,30,36)，(此时<f(x),fy)>=16+40+72+40+100+180+72+180+324=1024。

似乎还能计算,但是如果将维数扩大到一个非常大数时候,计算起来可就不是一丁点问题了。

但是发现,K(x,y)=(<x,y>)^2

K(x, y) = (4+10+18)^2 = 32^2 = 1024

俩者相等，`K(x,y)=(<x,y>)^2=<f(x),f(y)>`，但是K(x,y)计算起来却比<f(x),f(y)>简单的多

也就是说只要用K(x,y)来计算,,效果和<f(x),f(y)>是一样的,但是计算效率却大幅度提高了，如:K(x,y)是O(n),而<f(x),f(y)>是0(n^2)。

所以使用核函数的好处就是，可以在一个低维空间去完成高维度(或者无限维度)样本内积的计算，比如K(xy)=(4+10+18)^2的3D空间对比<f(x),f(y)> = 16+40+72+40+100+180+72+180+324的9D空间。

#### 高斯核函数

> 最常用的，最好用的核函数

一维的高斯![1604673996642](assets/1604673996642.png)

![1604674036293](assets/1604674036293.png)

二维的高斯![1604674075933](assets/1604674075933.png)

![1604674100250](assets/1604674100250.png)

公式：![1604674144599](assets/1604674144599.png)

- 看起来像两个样本点之间的距离的度量，如果X和Y很相似，那么结果也就是1，如果不相似那就是0。

- 这样做的好处，特征多了无穷个，得到无穷维。

  ![1604674300763](assets/1604674300763.png)

#### 参数的影响

高斯核函数看起来不错，但是它对参数是极其敏感的，效果差异也很大

σ^2 = 0.5![1604674774921](assets/1604674774921.png)

> σ越小，顶部越尖，我们计算的是样本到样本点的距离，也就是尖尖到底部的距离，底部到顶部变化越快，层次更分明，那么特征也就越明显，识别的越容易，风险也会比较高

σ^2 = 3![1604674941094](assets/1604674941094.png)

> σ越大，层次越平衡，也就是大家的特征都差不多，那么识别也越不容易，但是风险也相对低

决策边界如下图，σ越小，切分越厉害，越容易过拟合

![1604675119736](assets/1604675194246.png)![1604675119736](assets/1604675119736.png)![1604675249968](assets/1604675249968.png)

> 原σ在下面，注意上面的公式||x - x'||^2 / 2σ^2，这里移上去了，所以前面加上负号，第一个是负1，第二个是负10，第三个是负100



### 熵和激活函数

#### 熵的概念

- 物体内部的混乱程度。（一件事发生的不确定性）
- ![1604729221090](assets/1604729221090.png)

- 所有的概率值都在0-1之间，那么最终H(X)必然也是一个正数

#### 熵的大小意味着

- 假如有100个商品，那么选到某个商品的概率非常低，而如果商品只有几个，那么选到某个商品的概率非常高
- 如公式，商品越多，所有log后的值就越高，且公式是求和，那么值就更大

想象一个分类任务，我们希望得到如下的那种结果

- A[1,1,1,1,1,1,1,1,1,1,1]
- B[1,2,3,4,5,3,4,2,2,1,1]

显然A集合才是我们希望得到的结果，它的熵值表现是非常小的。

比如我们手上有一份数据，有两个指标性别和资产，判断是否给该用户贷款，性别和资产分组完后，如果资产熵值小，那么我们可以认为资产对是否可以贷款的影响更重要。

#### 激活函数（Sigmoid函数）

- Sigmoid是常用的非线性激活函数
- 能够把连续值压缩到0-1区间，不断的下降
- 缺点：杀死梯度，非原点中心对称

![1604731377718](assets/1604731377718.png)

解决当正负样本不好分类的时候，无法用线性分割，那么用一个概率值去定义一个样本是否是正负，比如大于0.5定义为正，否则是负。

又如5分类任务时，我们可以输出成以下形式

| 样本 | 类别1 | 类别2 | 类别3 | 类别4 | 类别5 |
| ---- | ----- | ----- | ----- | ----- | ----- |
| A    | 0.1   | 0.9   | 0.3   | 0.6   | 0.1   |
| B    | 0.9   | 0.1   | 0.1   | 0.1   | 0.1   |
| C    | 0.1   | 0.1   | 0.1   | 0.1   | 0.6   |

如上所示，我们可以认为A是类别2和类别4的，B是类别1的，C是类别5的。